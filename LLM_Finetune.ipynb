{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLLmu21OrW6E"
   },
   "source": [
    "# Fine-Tuning TinyLlama with LoRA on Databricks Dolly-15k\n",
    "\n",
    "This project evaluates the performance of **TinyLlama**, a lightweight variant of **LLaMA**, fine-tuned using **LoRA** (Low-Rank Adaptation) on the [**databricks/databricks-dolly-15k**](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset via Hugging Face.\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "- **Model**: `TinyLlama/TinyLlama-1.1B-Chat-v1.0`\n",
    "- **Fine-tuning**: Applied **LoRA**, targeting transformer attention layers (`q_proj`, `v_proj`) for efficient parameter tuning.\n",
    "- **Dataset**: Contains 15,000 instruction-response pairs across 8 task categories.\n",
    "- **Prompting**: Standardized format using labeled `Instruction`, `Context`, and `Response` blocks.\n",
    "- **Decoding**: Used **beam search** for more fluent and deterministic generation.\n",
    "- **Evaluation**: Compared base and fine-tuned models using **BLEU**, **ROUGE-L**, and **BERTScore**, both globally and per task category.\n",
    "- **Tokenization**: All inputs were tokenized using the tokenizer aligned with TinyLlama's pretraining setup.\n",
    "\n",
    "---\n",
    "\n",
    "## Results Summary\n",
    "\n",
    "| Metric       | Overall Trend with LoRA |\n",
    "|--------------|--------------------------|\n",
    "| **BLEU**     | Mixed; modest gain in open-ended tasks, decline in structured tasks |\n",
    "| **ROUGE-L**  | Minimal change |\n",
    "| **BERTScore**| Minimal change |\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "While LoRA fine-tuning on TinyLlama shows modest improvements or stability in open-ended and creative tasks, it underperforms or offers no improvement in structured, high-precision tasks like classification or extraction.\n",
    "Overall, there was no clear improvement, reinforcing that task-specific adaptation may be necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets peft accelerate evaluate bitsandbytes\n",
    "!pip install -q --upgrade datasets fsspec\n",
    "!pip install -U transformers\n",
    "!pip install rouge_score nltk absl-py\n",
    "!pip install bert-score\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling,Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig\n",
    "from bert_score import score as bertscore\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "#suppress logging and progress bars\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from huggingface_hub import hf_hub_download\n",
    "import transformers.utils.logging as hf_logging\n",
    "disable_progress_bar()\n",
    "hf_logging.set_verbosity_error()\n",
    "hf_logging.disable_progress_bar()\n",
    "\n",
    "output_dir = \"/workspace/tinyllama-dolly-lora\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXVQD2f1Ownd"
   },
   "source": [
    "## Dataset: databricks/databricks-dolly-15k\n",
    "\n",
    "This project uses the [Dolly 15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset, released by Databricks, for instruction-tuned fine-tuning of small language models.\n",
    "The Dolly 15k dataset contains **15,000 instruction-following records** that  consist of the following fields:\n",
    "\n",
    "| Field        | Description                                                                 |\n",
    "|--------------|-----------------------------------------------------------------------------|\n",
    "| `instruction`| The task description or instruction given to the model.                    |\n",
    "| `context`    | Optional additional information or input required for the task.            |\n",
    "| `response`   | The expected model output/answer to the instruction.                       |\n",
    "| `category`   | The category of task (e.g., open QA, summarization, classification, etc.). |\n",
    "\n",
    "The `context` field is optional and may be empty for many examples. When present, it provides useful supporting information for the instruction. The model should learn to generate appropriate responses both with and without additional context.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dolly 15k dataset from Hugging Face (created by Databricks)\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "\n",
    "# Split the dataset into training and validation sets (90% train, 10% validation)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"test\"]\n",
    "\n",
    "# Display dataset structure and samples\n",
    "print(\"\\nDataset splits and sizes:\")\n",
    "print(dataset)\n",
    "print(f\"\\nTrain size: {len(train_data)} samples\")\n",
    "print(f\"Validation size: {len(val_data)} samples\")\n",
    "\n",
    "for i in range(1):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(\"Instruction:\", train_data[i][\"instruction\"])\n",
    "    print(\"Context    :\", train_data[i][\"context\"])\n",
    "    print(\"Response   :\", train_data[i][\"response\"])\n",
    "    print(\"Category   :\", train_data[i][\"category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5zYLA1WPfmI"
   },
   "source": [
    "### Category Distribution in Training Data\n",
    "\n",
    "To understand the types of tasks represented in the Dolly 15k dataset, we analyze the frequency of each `category` in the training split. The dataset includes the following task categories:\n",
    "- open_qa\n",
    "- general_qa\n",
    "- classification\n",
    "- brainstorming\n",
    "- closed_qa   \n",
    "- information_extraction\n",
    "- summarization\n",
    "- creative_writing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all category labels from the training set\n",
    "categories = [example[\"category\"] for example in dataset[\"train\"]]\n",
    "\n",
    "# Count occurrences of each category\n",
    "category_counts = Counter(categories)\n",
    "\n",
    "print(\"Number of examples per category in the training set:\\n\")\n",
    "for category, count in sorted(category_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"{category:<25}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CObyRZgkQ1KC"
   },
   "source": [
    "### Prompt Formatting for Instruction Tuning\n",
    "\n",
    "Before fine-tuning the model, we convert each example in the dataset into a structured text prompt. Including labeled sections `Instruction`, `Context`, and `Response` helps the model distinguish between input and expected output clearly.\n",
    "\n",
    "\n",
    "**When `context` is present and non-empty:**\n",
    "\n",
    "- Instruction: [instruction text]  \n",
    "- Context: [context text]  \n",
    "- Response: [response text]\n",
    "\n",
    "**When `context` is missing or empty:**\n",
    "\n",
    "- Instruction: [instruction text]  \n",
    "- Response: [response text]\n",
    "\n",
    "\n",
    " The formatting is applied using the `.map()` method to both the training and validation splits.\n",
    "This structured prompt format is compatible with supervised fine-tuning setups commonly used in instruction-tuned LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format each example into a structured prompt for instruction tuning\n",
    "def format_prompt(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    context = example[\"context\"]\n",
    "    response = example[\"response\"]\n",
    "\n",
    "    # Include context only if it's non-empty and non-whitespace\n",
    "    if context and context.strip():\n",
    "        prompt = (\n",
    "            f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "            f\"### Context:\\n{context}\\n\\n\"\n",
    "            f\"### Response:\\n{response}\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "            f\"### Response:\\n{response}\"\n",
    "        )\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Apply the formatting function to both train and validation splits\n",
    "formatted_train = train_data.map(format_prompt)\n",
    "formatted_val = val_data.map(format_prompt)\n",
    "\n",
    "# Preview one formatted example\n",
    "print(formatted_train[0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkJPjhnGR8Ps"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "To prepare the formatted prompts for model training, we tokenize the text using the Hugging Face `AutoTokenizer`. Tokenization converts raw text into input token IDs and attention masks, which the model uses during training.\n",
    "We use the tokenizer from the `TinyLlama/TinyLlama-1.1B-Chat-v1.0` model to ensure compatibility. Since this model does not define a separate padding token, we set the padding token to be the same as the end-of-sequence (EOS) token.\n",
    "\n",
    "We apply both padding and truncation during tokenization:\n",
    "\n",
    "- **Truncation**: If a text sequence exceeds the specified `max_length`, it is truncated from the end.\n",
    "- **Padding**: All sequences are padded to a fixed length using `padding=\"max_length\"`.\n",
    "- **Max Length**: We set `max_length=512`, which means all tokenized inputs will be exactly 512 tokens long. This value can be reduced (e.g., to 256) for faster training or to fit within memory constraints.\n",
    "\n",
    "\n",
    "The tokenizer outputs two key fields:\n",
    "\n",
    "- `input_ids`: The sequence of token IDs corresponding to the prompt text.\n",
    "- `attention_mask`: A binary mask indicating which tokens are real (`1`) and which are padding (`0`).\n",
    "\n",
    "These tokenized outputs are then converted to PyTorch tensors and used as input to the model for supervised instruction fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model ID and corresponding tokenizer\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "#Use the EOS token as pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Apply tokenization to both training and validation sets\n",
    "tokenized_train = formatted_train.map(tokenize, batched=True)\n",
    "tokenized_val = formatted_val.map(tokenize, batched=True)\n",
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xc3X48IHTS_a"
   },
   "source": [
    "### Parameter-Efficient Fine-Tuning with LoRA\n",
    "\n",
    "To fine-tune the `TinyLlama` model efficiently, we apply **LoRA (Low-Rank Adaptation)** using the `peft` library.\n",
    "LoRA fine-tunes large pre-trained models by injecting low-rank trainable matrices into selected weight layers, typically in attention projections. Instead of updating the original weight matrix directly, LoRA adds a learned low-rank update. When fine-tuning with LoRA, only 1,126,400 out of 1,101,174,784 parameters are trainable, which is just 0.10% of the total model size.\n",
    "\n",
    "In LoRA, the weight matrix $W$ is frozen, and a low-rank decomposition is added:\n",
    "\n",
    "$$W_{\\text{LoRA}} = W + \\alpha \\cdot B A$$\n",
    "where:\n",
    "- $ A \\in \\mathbb{R}^{r \\times d_{\\text{in}}} $ and $ B \\in \\mathbb{R}^{d_{\\text{out}} \\times r} $ are trainable matrices,\n",
    "- $ r $ is the rank of the low-rank update (a hyperparameter),\n",
    "- $\\alpha$ is the scaling factor that controls the magnitude of the update.\n",
    "\n",
    "This formulation allows fine-tuning only the smaller matrices $A$ and $B$, reducing the number of trainable parameters:\n",
    "\n",
    "$$\n",
    "\\text{Trainable Params (LoRA)} = r \\cdot (d_{\\text{in}} + d_{\\text{out}})\n",
    "$$\n",
    "\n",
    "compared to:\n",
    "\n",
    "$$\n",
    "\\text{Trainable Params (Full)} = d_{\\text{in}} \\cdot d_{\\text{out}}\n",
    "$$\n",
    "\n",
    "The product $B A x$ represents a low-rank approximation of the update to $W x$.\n",
    "We define the following configuration when wrapping the base model:\n",
    "\n",
    "- **`r = 8`**: This sets the rank of the low-rank decomposition matrices injected into the model.\n",
    "\n",
    "- **`lora_alpha = 16`**: Typically set to 2x or 4× the value of `r`.\n",
    "\n",
    "- **`target_modules = [\"q_proj\", \"v_proj\"]`**: These refer to the query and value projection layers in the model’s attention mechanism.\n",
    "\n",
    "- **`lora_dropout = 0.05`**:  Dropout applied to LoRA weights during training for regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                                 # Rank of the low-rank decomposition\n",
    "    lora_alpha=16,                       # Scaling factor for the LoRA updates\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Attention projection layers to inject LoRA into\n",
    "    lora_dropout=0.05,                   # Dropout applied to LoRA layers\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM         # Specify the task type as causal language modeling\n",
    ")\n",
    "\n",
    "# Inject LoRA adapters into the base model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print the number of trainable parameters (only LoRA layers are trainable)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KufqgU8jVAKp"
   },
   "source": [
    "### Training Setup with Hugging Face Transformers\n",
    "\n",
    "We use the Hugging Face `transformers` library to train a causal language model. The training process is managed using the built-in `Trainer` API and\n",
    "is organized into three main components:\n",
    "\n",
    "#### 1. `TrainingArguments`\n",
    "\n",
    "This block defines the configuration for the training run, including output paths, batch sizes, number of epochs, logging and evaluation strategy, precision (fp16), and how the best model is selected.\n",
    "#### 2. `DataCollatorForLanguageModeling`\n",
    "\n",
    "This component dynamically batches the tokenized inputs during training. Since we are training a causal language model, masked language modeling (MLM) is disabled.\n",
    "\n",
    "#### 3. `Trainer`\n",
    "\n",
    "The Hugging Face `Trainer` implements the full training loop. It uses the configuration from `TrainingArguments` and the `data_collator` to run the training process end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=32,             # Batch size per device for training\n",
    "    per_device_eval_batch_size=32,              # Batch size per device for evaluation\n",
    "    num_train_epochs=5,                         # Number of training epochs\n",
    "    learning_rate=2e-4,                         # Learning rate\n",
    "    logging_steps=250,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    fp16=True,                                  # Use float16 precision\n",
    "    load_best_model_at_end=True,                # Load the best model (based on eval metric) after training\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Masked Language Modeling is disabled for causal LM tasks\n",
    ")\n",
    "\n",
    "# Set up the Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-hWcBYHWOwF"
   },
   "source": [
    "### Model Training\n",
    "\n",
    "We initiate the training process using the Hugging Face `Trainer`. This step begins fine-tuning the model on the tokenized and formatted Dolly dataset using the configuration defined earlier.\n",
    "The training process takes approximately **1 hour** on an **NVIDIA A100 PCIe GPU**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start model training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tbi94jaCV79"
   },
   "source": [
    "### Training Loss Visualization\n",
    "\n",
    "The following plots show the training and validation loss over time based on the logged trainer state:\n",
    "\n",
    "- **Left Plot**: Loss vs. Training Step  \n",
    "- **Right Plot**: Loss vs. Epoch\n",
    "\n",
    "From the plots, we observe that the training loss is steadily decreasing.\n",
    "Validation loss plateaus after a few epochs, suggesting that the model's performance on unseen data is converging to its generalization limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state from saved checkpoint\n",
    "log_file = \"/workspace/tinyllama-dolly-lora/checkpoint-2115/trainer_state.json\"\n",
    "with open(log_file, \"r\") as f:\n",
    "    state = json.load(f)\n",
    "\n",
    "# Collect losses and steps/epochs\n",
    "train_steps, train_losses_step = [], []\n",
    "eval_steps, eval_losses_step = [], []\n",
    "train_epochs, train_losses_epoch = [], []\n",
    "eval_epochs, eval_losses_epoch = [], []\n",
    "\n",
    "for entry in state[\"log_history\"]:\n",
    "    if \"loss\" in entry:\n",
    "        if \"step\" in entry:\n",
    "            train_steps.append(entry[\"step\"])\n",
    "            train_losses_step.append(entry[\"loss\"])\n",
    "        if \"epoch\" in entry:\n",
    "            train_epochs.append(entry[\"epoch\"])\n",
    "            train_losses_epoch.append(entry[\"loss\"])\n",
    "    if \"eval_loss\" in entry:\n",
    "        if \"step\" in entry:\n",
    "            eval_steps.append(entry[\"step\"])\n",
    "            eval_losses_step.append(entry[\"eval_loss\"])\n",
    "        if \"epoch\" in entry:\n",
    "            eval_epochs.append(entry[\"epoch\"])\n",
    "            eval_losses_epoch.append(entry[\"eval_loss\"])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "# Plot loss vs. training step\n",
    "axs[0].plot(train_steps, train_losses_step, marker=\"o\", label=\"Training Loss\")\n",
    "axs[0].plot(eval_steps, eval_losses_step, marker=\"x\", label=\"Validation Loss\")\n",
    "axs[0].set_xlabel(\"Training Step\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].set_title(\"Loss vs. Step\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot loss vs. epoch\n",
    "axs[1].plot(train_epochs, train_losses_epoch, marker=\"o\", label=\"Training Loss\")\n",
    "axs[1].plot(eval_epochs, eval_losses_epoch, marker=\"x\", label=\"Validation Loss\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "axs[1].set_title(\"Loss vs. Epoch\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6KFWW0aDOQR"
   },
   "source": [
    "### Comparing Base Model vs. LoRA Fine-Tuned Model\n",
    "\n",
    "In this step, we compare the performance of the original (pretrained) base model with the LoRA fine-tuned version.\n",
    "\n",
    "- We use the **TinyLlama/TinyLlama-1.1B-Chat-v1.0** model as the base.\n",
    "- We load the **LoRA fine-tuned checkpoint** and apply it to the same base model architecture.\n",
    "\n",
    "This setup allows us to run evaluation on the same validation data and directly compare the loss between the base and the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ID and LoRA checkpoint path\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "checkpoint_path = \"/workspace/tinyllama-dolly-lora/checkpoint-2115\"\n",
    "\n",
    "tokenized_val = tokenized_val.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the base model (no fine-tuning)\n",
    "base_model_clean = AutoModelForCausalLM.from_pretrained(model_id).to(\"cuda\")\n",
    "\n",
    "# Load the fine-tuned LoRA model by merging LoRA weights onto a base model\n",
    "lora_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "lora_model = PeftModel.from_pretrained(lora_model, checkpoint_path).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PbMaNscDgam"
   },
   "source": [
    "### Evaluation: Base Model vs. LoRA Fine-Tuned Model\n",
    "\n",
    "We evaluate both the original base model and the LoRA fine-tuned model on the same validation set to compare their performance. Loss values represent the average cross-entropy loss over the validation set.\n",
    "\n",
    "\n",
    "| Model             | Eval Loss | Notes                          |\n",
    "|------------------|-----------|--------------------------------|\n",
    "| Base Model        | 2.018     | Pretrained only, no fine-tuning |\n",
    "| LoRA Fine-Tuned   | 1.591     | Fine-tuned using LoRA on Dolly dataset |\n",
    "\n",
    "\n",
    "The LoRA fine-tuned model shows a **significantly lower evaluation loss** compared to the base model, indicating improved performance on the validation set after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Trainer for the base model\n",
    "trainer_base = Trainer(\n",
    "    model=base_model_clean,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Create a Trainer for the LoRA fine-tuned model\n",
    "trainer_lora = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Evaluate both models on the same validation set and print losses\n",
    "print(\"Base model eval loss:\", trainer_base.evaluate()[\"eval_loss\"])\n",
    "print(\"LoRA model eval loss:\", trainer_lora.evaluate()[\"eval_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwOWMhnDErQ1"
   },
   "source": [
    "### Prompt Formatting and Response Generation\n",
    "\n",
    "This section describes how we structure prompts and generate responses from a fine-tuned language model using Hugging Face Transformers.\n",
    "\n",
    "#### Prompt Formatting (`format_prompt`)\n",
    "\n",
    "The `format_prompt` function constructs a structured prompt based on an instruction and optional input (or context).\n",
    "This formatting mirrors the structure used during supervised fine-tuning.\n",
    "The prompt follows one of the following formats:\n",
    "\n",
    "- **With input/context:** `### Instruction: [instruction]`, `### Input: [context]`, `### Response:`\n",
    "- **Without input/context:** `### Instruction: [instruction]`, `### Response:`\n",
    "\n",
    "\n",
    "#### Response Generation (`generate_response`)\n",
    "\n",
    "The `generate_response` function uses the model to generate a textual output given a prompt. It leverages the Hugging Face `generate()` method **beam search** with 5 beams. Beam search explores multiple likely continuations at each step and keeps the top `num_beams` candidates. This helps find higher-quality, more coherent responses compared to greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format an instruction-following prompt for generation\n",
    "def format_prompt(instruction, input_text=\"\"):\n",
    "    if input_text:\n",
    "        return (\n",
    "            f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "            f\"### Input:\\n{input_text}\\n\\n\"\n",
    "            f\"### Response:\\n\"\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "            f\"### Response:\\n\"\n",
    "        )\n",
    "\n",
    "# Generate a response from the model given a prompt\n",
    "def generate_response(model, prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=5,              # Use beam search\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4gBb9JxFXmR"
   },
   "source": [
    "### Prompt-Based Evaluation: Base Model vs. LoRA Fine-Tuned Model\n",
    "\n",
    "To qualitatively evaluate the effect of fine-tuning, we compare the outputs of the base model and the LoRA fine-tuned model across a small set of prompts. One example is shown below, with additional examples included in the cell output.\n",
    "\n",
    "### Example:\n",
    "\n",
    "**Instruction:**  \n",
    "Explain black holes in simple terms.\n",
    "\n",
    "---\n",
    "\n",
    "**Base Model Output:**\n",
    "\n",
    "A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it. Black holes are formed when a massive star runs out of fuel and collapses under its own gravity. The collapse of the star creates a singularity, a point of infinite density and curvature, which is the point at which space and time become indistinguishable. Once a star has collapsed to this point, there is no escape for anything\n",
    "\n",
    "---\n",
    "\n",
    "**LoRA Fine-Tuned Model Output:**\n",
    "\n",
    "A black hole is a region of spacetime where the gravitational force is so strong that nothing, not even light, can escape from it. This is because the force of gravity is proportional to the square of the distance between the object and the event horizon, which is the point at which the curvature of space-time becomes so great that light can no longer escape. Black holes are thought to be formed when a massive object, such as a star, collapses under its own gravity.\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison:**\n",
    "\n",
    "Both models provide technically accurate answers. However, the LoRA fine-tuned model delivers a more complete and fluent explanation. The base model's response is informative but cuts off abruptly, while the fine-tuned version includes a more polished ending. This suggests that fine-tuning helps improve the model's coherence and completeness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompts for evaluating base and fine-tuned models\n",
    "prompts = [\n",
    "    \"Explain black holes in simple terms.\",\n",
    "    \"Give me 3 pros and 3 cons of remote work.\",\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"Why is exercise important for mental health?\",\n",
    "    \"Summarize the story of Little Red Riding Hood in one paragraph.\"\n",
    "]\n",
    "\n",
    "# Generate and compare responses from both models\n",
    "for i, instruction in enumerate(prompts):\n",
    "    formatted = format_prompt(instruction)\n",
    "\n",
    "    print(f\"\\n=== Prompt {i+1} ===\")\n",
    "    print(\"Prompt:\")\n",
    "    print(formatted)\n",
    "\n",
    "    print(\"\\nBase Model Response:\")\n",
    "    print(generate_response(base_model_clean, formatted))\n",
    "\n",
    "    print(\"\\nLoRA Fine-Tuned Model Response:\")\n",
    "    print(generate_response(lora_model, formatted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OphSvTvVHV3M"
   },
   "source": [
    "### BLEU and ROUGE Scores\n",
    "\n",
    "We evaluate the base and LoRA fine-tuned models using the two metrics **BLEU** and **ROUGE-L**.\n",
    "\n",
    "#### BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "BLEU measures the overlap between n-grams in the model-generated output and the reference text. It is precision-oriented, meaning it evaluates how much of the generated text matches the reference. BLEU is commonly used for tasks like translation or structured generation, where exact phrasing matters.\n",
    "\n",
    "\n",
    "#### ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "ROUGE-L measures the longest common subsequence between the generated output and reference, capturing recall-oriented aspects of content overlap. It is especially useful for summarization and instruction following, where capturing the right information is more important than matching exact phrasing.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Results (on 100 validation samples)\n",
    "\n",
    "| Model          | BLEU Score | ROUGE-L Score |\n",
    "|----------------|------------|----------------|\n",
    "| Base Model     | 0.0684     | 0.1652         |\n",
    "| LoRA Fine-Tuned| 0.0637     | 0.1537         |\n",
    "\n",
    "---\n",
    "\n",
    "Both models achieve similar scores, but the base model slightly outperforms the LoRA fine-tuned model on both BLEU and ROUGE-L metrics in this evaluation. This result may reflect that the LoRA model, while more fluent and expressive in some qualitative examples, sometimes diverges more from reference wording.\n",
    "Overall, this suggests that LoRA fine-tuning may require additional tuning to consistently improve in metrics like BLEU and ROUGE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLEU and ROUGE metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Evaluate a model on a subset of the validation dataset\n",
    "def evaluate_model(model, dataset, n_samples=100):\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Generate predictions for the first n_samples examples\n",
    "    for example in dataset.select(range(n_samples)):\n",
    "        prompt = format_prompt(example[\"instruction\"], example.get(\"context\", \"\"))\n",
    "        target = example[\"response\"]\n",
    "\n",
    "        output = generate_response(model, prompt)\n",
    "        predictions.append(output.strip())\n",
    "        references.append(target.strip())\n",
    "\n",
    "    # Compute BLEU and ROUGE-L scores\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return {\"BLEU\": bleu_score[\"bleu\"], \"ROUGE-L\": rouge_score[\"rougeL\"]}\n",
    "\n",
    "# Evaluate base model\n",
    "print(\"Evaluating Base Model...\")\n",
    "base_scores = evaluate_model(base_model_clean, formatted_val)\n",
    "print(\"Base Model:\", base_scores)\n",
    "\n",
    "# Evaluate LoRA fine-tuned model\n",
    "print(\"\\nEvaluating LoRA Model...\")\n",
    "lora_scores = evaluate_model(lora_model, formatted_val)\n",
    "print(\"LoRA Model:\", lora_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHOJSJN9IBrq"
   },
   "source": [
    "### Semantic Evaluation with BERTScore\n",
    "\n",
    "In addition to BLEU and ROUGE, we use **BERTScore** to evaluate the semantic similarity between model-generated responses and reference answers.\n",
    "BERTScore compares each token in the prediction and reference using contextualized embeddings from BERT. It computes three metrics:\n",
    "\n",
    "- **Precision**: How much of the predicted meaning is aligned with the reference.\n",
    "- **Recall**: How much of the reference meaning is captured by the prediction.\n",
    "- **F1**: The harmonic mean of precision and recall.\n",
    "\n",
    "This metric is especially useful when the wording between the prediction and reference differs, but the underlying meaning is preserved.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Results (on 100 validation samples)\n",
    "\n",
    "| Model           | BERTScore Precision | BERTScore Recall | BERTScore F1 |\n",
    "|-----------------|---------------------|------------------|--------------|\n",
    "| Base Model      | 0.8235              | 0.8678           | 0.8445       |\n",
    "| LoRA Fine-Tuned | 0.8169              | 0.8689           | 0.8414       |\n",
    "\n",
    "---\n",
    "\n",
    "- The **base model** achieves slightly higher precision and F1 scores, indicating closer alignment of generated content with reference phrasing.\n",
    "- The **LoRA fine-tuned model** shows marginally higher recall, suggesting it captures more of the reference meaning, even if not always phrased the same way.\n",
    "\n",
    "These results align with earlier qualitative observations: the LoRA model tends to be more expressive and expansive, while the base model may hew more closely to the original response structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a model using BERTScore on a subset of the dataset\n",
    "def evaluate_with_bertscore(model, dataset, n_samples=100):\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Generate predictions and collect corresponding references\n",
    "    for example in dataset.select(range(n_samples)):\n",
    "        prompt = format_prompt(example[\"instruction\"], example.get(\"context\", \"\"))\n",
    "        reference = example[\"response\"].strip()\n",
    "        output = generate_response(model, prompt).strip()\n",
    "\n",
    "        predictions.append(output)\n",
    "        references.append(reference)\n",
    "\n",
    "    # Compute BERTScore\n",
    "    P, R, F1 = bertscore(predictions, references, lang=\"en\", verbose=True)\n",
    "    return {\n",
    "        \"BERTScore Precision\": P.mean().item(),\n",
    "        \"BERTScore Recall\": R.mean().item(),\n",
    "        \"BERTScore F1\": F1.mean().item()\n",
    "    }\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Evaluating Base Model with BERTScore...\")\n",
    "base_bert = evaluate_with_bertscore(base_model_clean, formatted_val)\n",
    "\n",
    "print(\"\\nEvaluating LoRA Model with BERTScore...\")\n",
    "lora_bert = evaluate_with_bertscore(lora_model, formatted_val)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(\"Base Model:\", base_bert)\n",
    "print(\"LoRA Model:\", lora_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZzc2hCGIqIq"
   },
   "source": [
    "### Category-wise Evaluation\n",
    "\n",
    "To analyze how the model performs across different task types, we evaluate its outputs by category using a subset of examples from each.\n",
    "For each category, we select up to 50 examples.\n",
    "The model generates a response, which is compared to the reference using the  metrics: **BLEU**, **ROUGE-L**, and **BERTScore (F1)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Evaluate model performance by category\n",
    "def evaluate_by_category(model, dataset, categories, max_samples_per_cat=50):\n",
    "    results = {}\n",
    "\n",
    "    for category in categories:\n",
    "        # Filter dataset by category\n",
    "        filtered = dataset.filter(lambda x: x[\"category\"] == category)\n",
    "        subset = filtered.select(range(min(max_samples_per_cat, len(filtered))))\n",
    "\n",
    "        predictions = []\n",
    "        references = []\n",
    "\n",
    "        # Generate predictions for each example in the category\n",
    "        for example in tqdm(subset, desc=f\"Evaluating {category}\"):\n",
    "            prompt = format_prompt(example[\"instruction\"], example.get(\"context\", \"\"))\n",
    "            reference = example[\"response\"].strip()\n",
    "            prediction = generate_response(model, prompt).strip()\n",
    "\n",
    "            predictions.append(prediction)\n",
    "            references.append(reference)\n",
    "\n",
    "        # Compute BLEU score\n",
    "        bleu = bleu_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=[[ref] for ref in references]\n",
    "        )[\"bleu\"]\n",
    "\n",
    "        # Compute ROUGE-L score\n",
    "        rouge = rouge_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=references\n",
    "        )[\"rougeL\"]\n",
    "\n",
    "        # Compute BERTScore (F1)\n",
    "        _, _, f1 = bertscore(predictions, references, lang=\"en\", verbose=False)\n",
    "\n",
    "        # Store results for the category\n",
    "        results[category] = {\n",
    "            \"BLEU\": bleu,\n",
    "            \"ROUGE-L\": rouge,\n",
    "            \"BERTScore F1\": f1.mean().item()\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eFadGTXJM_w"
   },
   "source": [
    "### Category-wise Evaluation Results\n",
    "\n",
    "We evaluated both the base model and the LoRA fine-tuned model across different task categories using **BLEU**, **ROUGE-L**, and **BERTScore (F1)**. The table below summarizes the results:\n",
    "\n",
    "| Category               | BLEU (Base) | BLEU (LoRA) | ROUGE-L (Base) | ROUGE-L (LoRA) | BERT F1 (Base) | BERT F1 (LoRA) |\n",
    "|------------------------|-------------|-------------|----------------|----------------|----------------|----------------|\n",
    "| closed_qa              | 0.090       | 0.085       | 0.204          | 0.184          | 0.860          | 0.857          |\n",
    "| creative_writing       | 0.008       | 0.013       | 0.148          | 0.146          | 0.834          | 0.835          |\n",
    "| general_qa             | 0.039       | 0.046       | 0.171          | 0.172          | 0.845          | 0.848          |\n",
    "| brainstorming          | 0.038       | 0.041       | 0.163          | 0.153          | 0.839          | 0.840          |\n",
    "| open_qa                | 0.036       | 0.041       | 0.168          | 0.155          | 0.840          | 0.836          |\n",
    "| information_extraction | 0.086       | 0.081       | 0.193          | 0.187          | 0.845          | 0.841          |\n",
    "| classification         | 0.099       | 0.072       | 0.244          | 0.184          | 0.855          | 0.843          |\n",
    "| summarization          | 0.138       | 0.134       | 0.279          | 0.272          | 0.871          | 0.870          |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- **Summarization** yields the highest scores across all metrics for both models. This suggests both models are relatively strong in this category.\n",
    "- **Creative writing** shows very low BLEU and ROUGE-L scores. BERTScore remains reasonably high, indicating semantic similarity despite lexical differences.\n",
    "- **Classification** shows a noticeable drop in BLEU and ROUGE-L for the LoRA model compared to the base model.\n",
    "- **General QA and brainstorming** see slight improvements in BLEU and BERTScore under LoRA fine-tuning, suggesting better instruction adherence and fluency.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "LoRA fine-tuning leads to modest improvements or stability in some open-ended categories (e.g., QA, creative writing), while slightly underperforming in structured tasks like classification and information extraction. This reinforces the idea that **LoRA improves fluency and creativity**, but **may require further alignment or format constraints** for high-precision tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique categories from the validation set\n",
    "categories = list(set(example[\"category\"] for example in formatted_val))\n",
    "\n",
    "# Evaluate base model by category\n",
    "print(\"Evaluating Base Model...\")\n",
    "base_cat_results = evaluate_by_category(base_model_clean, formatted_val, categories)\n",
    "\n",
    "# Evaluate LoRA fine-tuned model by category\n",
    "print(\"\\nEvaluating LoRA Model...\")\n",
    "lora_cat_results = evaluate_by_category(lora_model, formatted_val, categories)\n",
    "\n",
    "table = []\n",
    "for cat in categories:\n",
    "    table.append([\n",
    "        cat,\n",
    "        f\"{base_cat_results[cat]['BLEU']:.3f}\",\n",
    "        f\"{lora_cat_results[cat]['BLEU']:.3f}\",\n",
    "        f\"{base_cat_results[cat]['ROUGE-L']:.3f}\",\n",
    "        f\"{lora_cat_results[cat]['ROUGE-L']:.3f}\",\n",
    "        f\"{base_cat_results[cat]['BERTScore F1']:.3f}\",\n",
    "        f\"{lora_cat_results[cat]['BERTScore F1']:.3f}\",\n",
    "    ])\n",
    "\n",
    "headers = [\n",
    "    \"Category\",\n",
    "    \"BLEU (Base)\", \"BLEU (LoRA)\",\n",
    "    \"ROUGE-L (Base)\", \"ROUGE-L (LoRA)\",\n",
    "    \"BERT F1 (Base)\", \"BERT F1 (LoRA)\"\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers=headers, tablefmt=\"grid\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
